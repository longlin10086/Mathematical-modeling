## 第四章 评价类模型

> 为什么需要评价类模型？
>
> - 同一个问题，不同方案之间需要评价
> - 同一个问题，不同对象之间需要评价
> - 同一个问题，不同准则之间需要评价

### 4.1 层次分析法（ AHP ）

日常生活中往往会面临很多决策，层次分析法就是在对复杂决策问题的本质、影响因素及其内在关系等进行深入研究的基础上，利用较少的定量信息使决策的思维过程数学化，从而为多目标、多准则或无结构特性的复杂决策问题提供简便的决策方法。**是一种对难以完全定量的复杂系统做出决策的模型和方法。**

层次分析法大体可以分为以下四个步骤：

1. 建立层次结构模型
2. 构造判断（成对比较）矩阵
3. 层次单排序及其一致性检验
4. 层次总排序及其一致性检验

参考资料：[b站 北海数学建模层次分析法](https://www.bilibili.com/video/BV1Ja41117Af/?spm_id_from=333.337.search-card.all.click&vd_source=ee9a2683a5f602f2e565f5f62e1fa764)

#### 4.1.1 建立层次结构模型

将决策的目标、考虑的因素(决策准则)和决策对象按他们之间的相互关系分成最高层、中间层和最低层，绘制层次结构图。

- 最高层（目标层）：决策的目的、要解决的问题

- 中间层（准则层或指标层）：考虑的因素、决策的准则

- 最低层（方案层）：决策的备选方案

<img src=".\img\AHP.png" alt="AHP" style="zoom:150%;" />

#### 4.1.2 构造判断（成对比较）矩阵

| 标度       | 含义                                                         |
| ---------- | ------------------------------------------------------------ |
| 1          | 表示两个因素相比，具有同样重要性                             |
| 3          | 表示两个因素相比，一个因素比另一个因素稍微重要               |
| 5          | 表示两个因素相比，一个因素比另一个因素明显重要               |
| 7          | 表示两个因素相比，一个因素比另一个因素强烈重要               |
| 9          | 表示两个因素相比，一个因素比另一个因素极端重要               |
| 2，4，6，8 | 上述两响铃判断的中值                                         |
| 倒数       | 若因素 i 与 因素 j 比较判断的结果是 $a_{ij}$ ，则因素 j 与 i 的比较结果是 $a_{ji}=\dfrac{1}{a_{ij}}$ |

根据表中的判断标准即可绘制一个对比矩阵（将列元素作为被比较者）

但是，根据主观判断两两比较得出的比较矩阵可能存在冲突，需要经过一致性检验判断是否合理

#### 4.1.3 一致性检验

计算一致性比例 CR

- $CR=\dfrac{CI}{RI}$ ，其中 $CI=\dfrac{\lambda_{max}-n}{n-1}$ ，其中 $\lambda_{max}$ 为对比矩阵的最大特征值，$n$ 为指标数（即判断矩阵的行数）
- $RI$ 为平均随机一致性指标，通过查表可得

|  n   |  1   |  2   |  3   |  4   |  5   |  6   |  7   |  8   |  9   |  10  |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| $RI$ |  0   |  0   | 0.52 | 0.89 | 1.12 | 1.26 | 1.36 | 1.41 | 1.46 | 1.49 |

如果求出的 $CR$ 比某个非常小的数要小，比如 0.1 ，那么该判断矩阵通过了一致性检验，否则需要调整矩阵中的元素重新计算

#### 4.1.4 算数平均法计算权重

先将通过了一致性检验的判断矩阵**按列归一化**，接着对每一行分别求和，求和的结果除以 n ，得到的列向量就是权重向量

最后对所有准则进行加权求和，得到综合评分进行排名（进行加权的数据需要经过归一化处理）

### 4.2 熵权法

#### 4.2.1 熵的概念

- 信息论中，熵是对不确定性的一种度量，可以判断一个事件的**随机性和无序程度**
- 用熵值可以判断某个指标的离散程度，指标的离散程度越大，该指标对综合评价的影响越大

实际举例可为：某一门课 「不易拉开差距」，意味着这一门课对最终成绩影响很小，**相应混乱程度就低**，评价总成绩是可以给该门课**赋予低权重**

#### 4.2.2 熵权法的特点

熵权法是**根据数据本身建立评价体系**，完全客观，追求公平公正。

该方法适用于只有数据没有说明各因素的重要程度时，或缺乏评价的文献资料以及文献资料说法不一时。

但是，熵权法也有缺点。比如，在调查各个地区水质时，某地区含氧量指标更重要，而另一地区 PH 值更重要，这时便不太适合用熵权法进行评价。

#### 4.2.3 具体步骤

**第一步**，先将指标进行归一化处理（一共 n 个样本，m 个指标，$x_{ij}$ 为第 i 个样本的第 j 个指标的数值）：

- 正向指标（指标越大越好）：

$$
X_{ij}=\dfrac{x_{ij}-\min\{x_{1j},\ldots,x_{nj}\}}{\max\{x_{1j},\ldots,x_{nj}\}-\min\{x_{1j},\ldots,x_{nj}\}}
$$

- 负向指标（指标越小越好）：

$$
X_{ij}=\dfrac{\max\{x_{1j},\ldots,x_{nj}\}-x_{ij}}{\max\{x_{1j},\ldots,x_{nj}\}-\min\{x_{1j},\ldots,x_{nj}\}}
$$

- 进行比重计算：

$$
p_{ij}=\dfrac{X_{ij}}{\sum\limits_{i=1}^nX_{ij}}
$$

**第二步**，计算第 j 项指标的熵值
$$
e_j=-k\sum_{i=1}^np_{ij}\ln(p_{ij})
$$
其中 $k=\dfrac{1}{\ln n}>0$ ，所以满足 $e_j \ge 0$ ，如此计算出信息熵冗余度 $d_j=1-e_j$

**第三步**，计算各指标的权重 $w_j$，并由此得出各样本的综合得分 $S_i$

其中权重 $w_j$ 的计算方法：
$$
w_j=\dfrac{d_j}{\sum\limits_{j=1}^m d_j}
$$

$$
S_i=\sum_{j=1}^m w_j P_{ij}
$$

具体例子可看：[知乎 熵权法评价估计详细原理讲解（部分公式有误）](https://zhuanlan.zhihu.com/p/267259810)

### 4.3 TOPSIS法

参考资料：[TOPSIS(逼近理想解)算法原理详解与代码实现 - 知乎 ](https://zhuanlan.zhihu.com/p/266689519)

#### 4.3.1 数据预处理

- **指标正向化**

在处理数据时，有些指标的数据越大越好，有些则是越小越好，有些又是中间某个值或某段区间最好。

此时我们可以对其进行 「正向化处理」，使指标都可以项考试分数一样越大越好。

1. 极小型与极大型：如同上文的正向负向指标一样，采用 min-max 规约
2. 区间型：

$$
\widehat{x_i}=\begin{cases}
1-\dfrac{a-x_i}{M},\quad x_i<a \\
1, \quad a<x_i<b \\
1-\dfrac{x_i-b}{M}, \quad x_i>b
\end{cases}
$$

3. 中间型：

$$
\widehat{x_i}=1-\dfrac{x_i-x_{best}}{M}
$$

其中，式中出现的 M 为数据端点值与 **中间值\区间端点** 的最大距离

- **指标无量纲化（ Z-score 规约）**

为了消除不同数据指标量纲的影响，我们还有必要对已经正向化的矩阵进行标准化

#### 4.3.2 求解最佳样本

**第一步**，求出预处理后数据的正理想解（集合了所有指标的最优值）和负理想解（集合了两条指标的最差值）

**第二步**，将样本数据看作是在 n 维平面上的点，求出各点到正理想解与负理想解间的距离，如果某点到正理想解的距离最小，到负理想解的距离最大，那该样本即为最佳样本

### 4.4 CRITIC法

该方法是一种**比熵权法和标准离差法更好的客观赋权法**。它是基于评价指标的对比强度和指标之间的冲突性来综合衡量指标的客观权重。 考虑指标变异性大小的同时兼顾指标之间的相关性，并非数字越大就说明越重要，完全利用数据自身的客观属性进行科学评价。

#### 4.4.1 计算步骤

**第一步**，数据正向化与标准化，具体操作与前文一致，这里不加赘述（m 个待评对象，n 个评价指标，构成数据矩阵 $X=(x_{ij})_{m \times n}$ )

**第二步**，计算信息承载量，再这之前先计算两个中间量：对比度和矛盾性

1. 对比性（实质上就是标准差）

$$
\sigma_j=\sqrt{\dfrac{\sum\limits_{i=1}^m(x_{ij}-\overline{x_j})}{m-1}}
$$

2. 矛盾性：反映的是不同指标间的相关程度

$$
f_j=\sum_{i=1}^m(1-r_{ij})
$$

​	其中 $r_{ij}$ 表示指标 i 与指标 j 之间的相关系数（皮尔逊相关系数）
$$
r = \dfrac{\sum\limits_{i=1}^n(X_i-\overline{X})(Y_i-\overline{Y})}{\sqrt{\sum\limits_{i=1}^n(X_i-\overline{X})^2}\sqrt{\sum\limits_{i=1}^n(Y_i-\overline{Y})^2}}
$$

3. 信息承载量 $C_j=\sigma_jf_j$

**第三步**，计算权重
$$
w_j=\dfrac{C_j}{\sum\limits_{j=1}^nC_j}
$$

### 4.5 主成分分析（PCA）

在数据采集过程中可能会有大量数据之间存在相关关系，因此可以考虑将关系紧密的变量变成尽可能少的新变量，使这些新变量是两两不相关的，那么就可以用较少的综合指标分别代表存在于各个变量中的各类信息。主成分分析与因子分析就属于这类降维算法。

#### 4.5.1 数据降维

降维就是一种对高维度特征数据预处理方法。降维是将高维度的数据保留下最重要的一些特征，去除噪声和不重要的特征，从而实现提升数据处理速度的目的。在实际的生产和应用中，降维在一定的信息损失范围内，可以为我们节省大量的时间和成本。降维也成为应用非常广泛的数据预处理方法。

数据降维有很多优点：

- 使得数据集更易使用。
- 降低算法的计算开销。
- 去除噪声。

降维的算法很多，比如：奇异值分解（ SVD ）、主成分分析（ PCA ）、因子分析（ FA ）、独立成分分析（ICA）

#### 4.5.2 PCA的概念

PCA (Principal Component Analysis) 主要思想是将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征。PCA的工作就是从原始的空间中顺序地找一组相互正交的坐标轴，新的坐标轴的选择与数据本身是密切相关的。其中，第一个新坐标轴选择是原始数据中方差最大的方向，第二个新坐标轴选取是与第一个坐标轴正交的平面中使得方差最大的，第三个轴是与第1,2个轴正交的平面中方差最大的。依次类推，可以得到n个这样的坐标轴。通过这种方式获得的新的坐标轴，我们发现，大部分方差都包含在前面k个坐标轴中，后面的坐标轴所含的方差几乎为0。于是，我们可以忽略余下的坐标轴，只保留前面k个含有绝大部分方差的坐标轴。事实上，这相当于只保留包含绝大部分方差的维度特征，而忽略包含方差几乎为0的特征维度，实现对数据特征的降维处理。
